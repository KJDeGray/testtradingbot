{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca3cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'NewConfig' from 'c:\\\\Users\\\\Kristo\\\\Documents\\\\GitHub\\\\testtradingbot\\\\Groundup4\\\\NewConfig.py'>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "import importlib\n",
    "import Database as db\n",
    "import Config as c\n",
    "importlib.reload(db)\n",
    "importlib.reload(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ad447b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cf0bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_affiliation_state(df):\n",
    "    # Ensure symbols list\n",
    "    df['symbols'] = df['symbols'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "    texts = (df['headline'].fillna('') + '. ' + df['summary'].fillna('')).tolist()\n",
    "    article_embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "    # Build ticker index map\n",
    "    all_tickers = sorted({ticker for symbols in df['symbols'] for ticker in symbols})\n",
    "    ticker_to_idx = {ticker: i for i, ticker in enumerate(all_tickers)}\n",
    "\n",
    "    embed_dim = article_embeddings.shape[1]\n",
    "    ticker_sums = torch.zeros(len(all_tickers), embed_dim, device=article_embeddings.device)\n",
    "    ticker_counts = torch.zeros(len(all_tickers), device=article_embeddings.device)\n",
    "\n",
    "    for i, tickers in enumerate(df['symbols']):\n",
    "        emb = article_embeddings[i]\n",
    "        for ticker in tickers:\n",
    "            idx = ticker_to_idx[ticker]\n",
    "            ticker_sums[idx] += emb\n",
    "            ticker_counts[idx] += 1\n",
    "        print(f\"Processed {i+1}/{len(df)} articles\", end='\\r', flush=True)\n",
    "\n",
    "    ticker_embeddings = {t: ticker_sums[i] / ticker_counts[i] for t, i in ticker_to_idx.items()}\n",
    "\n",
    "    state = {\n",
    "        \"ticker_sums\": {t: ticker_sums[i] for t, i in ticker_to_idx.items()},\n",
    "        \"ticker_counts\": {t: int(ticker_counts[i].item()) for t, i in ticker_to_idx.items()},\n",
    "        \"ticker_embeddings\": ticker_embeddings,\n",
    "        \"all_tickers\": all_tickers\n",
    "    }\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "679518af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_article(article, state):\n",
    "    \"\"\"\n",
    "    Add a single article to existing scores_df and update state.\n",
    "    \n",
    "    article: dict with 'headline', 'summary', 'symbols' (list)\n",
    "    state: dict returned from build_affiliation_scores\n",
    "    \"\"\"\n",
    "    ticker_sums = state[\"ticker_sums\"]\n",
    "    ticker_counts = state[\"ticker_counts\"]\n",
    "    ticker_embeddings = state[\"ticker_embeddings\"]\n",
    "    all_tickers = state[\"all_tickers\"]\n",
    "\n",
    "    # Embed new article\n",
    "    text = article['headline'] + '. ' + article['summary']\n",
    "    embed = model.encode(text, convert_to_tensor=True)\n",
    "\n",
    "    # Handle new tickers\n",
    "    for ticker in article['symbols']:\n",
    "        if ticker not in ticker_sums:\n",
    "            ticker_sums[ticker] = torch.zeros(embed.shape)\n",
    "            ticker_counts[ticker] = 0\n",
    "            ticker_embeddings[ticker] = torch.zeros(embed.shape)\n",
    "            all_tickers.append(ticker)\n",
    "\n",
    "        ticker_sums[ticker] += embed\n",
    "        ticker_counts[ticker] += 1\n",
    "        ticker_embeddings[ticker] = ticker_sums[ticker] / ticker_counts[ticker]\n",
    "\n",
    "    # Compute new similarity row\n",
    "    row_scores = []\n",
    "    for ticker in all_tickers:\n",
    "        if ticker_counts[ticker] > 0:\n",
    "            score = torch.nn.functional.cosine_similarity(\n",
    "                embed.unsqueeze(0),\n",
    "                ticker_embeddings[ticker].unsqueeze(0)\n",
    "            ).item()\n",
    "        else:\n",
    "            score = 0.0\n",
    "        row_scores.append(score)\n",
    "\n",
    "    # Append new row\n",
    "    new_row = pd.DataFrame([row_scores], columns=all_tickers)\n",
    "\n",
    "    # Save updated state\n",
    "    state[\"ticker_sums\"] = ticker_sums\n",
    "    state[\"ticker_counts\"] = ticker_counts\n",
    "    state[\"ticker_embeddings\"] = ticker_embeddings\n",
    "    state[\"all_tickers\"] = all_tickers\n",
    "\n",
    "    return new_row, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8fd16f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 129347 tickers from cache.\n",
      "Last article: Benzinga examined the prospects for many investors&#39; favorite stocks over the last week — here&#39;s a look at some of our top stories. \n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "raw_ticker_data = db.load_cached_news()\n",
    "print(f\"Loaded {len(raw_ticker_data)} tickers from cache.\")\n",
    "raw_ticker_data = pd.DataFrame(raw_ticker_data)\n",
    "last_article = raw_ticker_data.iloc[-1] \n",
    "raw_ticker_data = raw_ticker_data.iloc[:-1] \n",
    "print(f\"Last article: {last_article[\"summary\"]}\")\n",
    "print(type(last_article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5dda24b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_affiliation_state() missing 1 required positional argument: 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m state = \u001b[43mbuild_affiliation_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_ticker_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: build_affiliation_state() missing 1 required positional argument: 'model'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "state = build_affiliation_state(raw_ticker_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8058d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores DataFrame shape: (129346, 8409)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"state shape: {state.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b6016",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Sparse pandas data (column A) not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_affiliation_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kristo\\Documents\\GitHub\\testtradingbot\\Groundup4\\Database.py:59\u001b[39m, in \u001b[36msave_affiliation_data\u001b[39m\u001b[34m(scores_df, state, base_filename)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_affiliation_data\u001b[39m(scores_df, state, base_filename = c.DATA_DIR):\n\u001b[32m     58\u001b[39m     scores_df_sparse = scores_df.astype(pd.SparseDtype(\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m, fill_value=\u001b[32m0\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[43mscores_df_sparse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase_filename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_scores.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzstd\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_state.pkl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     61\u001b[39m         dill.dump(state, f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kristo\\anaconda3\\envs\\stocks\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kristo\\anaconda3\\envs\\stocks\\Lib\\site-packages\\pandas\\core\\frame.py:3118\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3037\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3038\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3039\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3114\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3115\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3116\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3118\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kristo\\anaconda3\\envs\\stocks\\Lib\\site-packages\\pandas\\io\\parquet.py:482\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m impl = get_engine(engine)\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io.BytesIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kristo\\anaconda3\\envs\\stocks\\Lib\\site-packages\\pandas\\io\\parquet.py:191\u001b[39m, in \u001b[36mPyArrowImpl.write\u001b[39m\u001b[34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    189\u001b[39m     from_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33mpreserve_index\u001b[39m\u001b[33m\"\u001b[39m] = index\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfrom_pandas_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df.attrs:\n\u001b[32m    194\u001b[39m     df_metadata = {\u001b[33m\"\u001b[39m\u001b[33mPANDAS_ATTRS\u001b[39m\u001b[33m\"\u001b[39m: json.dumps(df.attrs)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kristo\\anaconda3\\envs\\stocks\\Lib\\site-packages\\pyarrow\\table.pxi:4795\u001b[39m, in \u001b[36mpyarrow.lib.Table.from_pandas\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kristo\\anaconda3\\envs\\stocks\\Lib\\site-packages\\pyarrow\\pandas_compat.py:594\u001b[39m, in \u001b[36mdataframe_to_arrays\u001b[39m\u001b[34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[39m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdataframe_to_arrays\u001b[39m(df, schema, preserve_index, nthreads=\u001b[32m1\u001b[39m, columns=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    586\u001b[39m                         safe=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    587\u001b[39m     (all_names,\n\u001b[32m    588\u001b[39m      column_names,\n\u001b[32m    589\u001b[39m      column_field_names,\n\u001b[32m    590\u001b[39m      index_column_names,\n\u001b[32m    591\u001b[39m      index_descriptors,\n\u001b[32m    592\u001b[39m      index_columns,\n\u001b[32m    593\u001b[39m      columns_to_convert,\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m      convert_fields) = \u001b[43m_get_columns_to_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m     \u001b[38;5;66;03m# NOTE(wesm): If nthreads=None, then we use a heuristic to decide whether\u001b[39;00m\n\u001b[32m    598\u001b[39m     \u001b[38;5;66;03m# using a thread pool is worth it. Currently the heuristic is whether the\u001b[39;00m\n\u001b[32m    599\u001b[39m     \u001b[38;5;66;03m# nrows > 100 * ncols and ncols > 1.\u001b[39;00m\n\u001b[32m    600\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nthreads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kristo\\anaconda3\\envs\\stocks\\Lib\\site-packages\\pyarrow\\pandas_compat.py:397\u001b[39m, in \u001b[36m_get_columns_to_convert\u001b[39m\u001b[34m(df, schema, preserve_index, columns)\u001b[39m\n\u001b[32m    394\u001b[39m name = _column_name_to_strings(name)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _pandas_api.is_sparse(col):\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    398\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSparse pandas data (column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    400\u001b[39m columns_to_convert.append(col)\n\u001b[32m    401\u001b[39m convert_fields.append(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: Sparse pandas data (column A) not supported."
     ]
    }
   ],
   "source": [
    "db.save_affiliation_data(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695aa41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_score_df, state = add_article(last_article, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4adc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         AA      AADI       AAL      AAPL      ABCM       ACB      ACCD  \\\n",
      "0 -0.028447 -0.011939 -0.057079  0.000126  0.049731  0.206803 -0.007461   \n",
      "\n",
      "       ACGL      ACLX       ACM  ...      XXII      YMAB        YY         Z  \\\n",
      "0  0.075077 -0.012231  0.094351  ...  0.090893 -0.031706 -0.008624  0.037699   \n",
      "\n",
      "        ZEV        ZG        ZH      ZLAB       ZM        ZS  \n",
      "0  0.001134  0.037699 -0.028476  0.005944  0.03895  0.001426  \n",
      "\n",
      "[1 rows x 913 columns]\n",
      "BNTX 0.34557077288627625\n"
     ]
    }
   ],
   "source": [
    "print(new_score_df)\n",
    "# Get the row as a Series\n",
    "row = new_score_df.iloc[0]\n",
    "# Max value\n",
    "max_val = row.max()\n",
    "# Column name of max\n",
    "max_col = row.idxmax()\n",
    "print(max_col , max_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
